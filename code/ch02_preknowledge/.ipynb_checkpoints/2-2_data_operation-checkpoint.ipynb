{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据操作\n",
    "### tensor操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.tensor的创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.1837e-39, 8.4490e-39, 1.0102e-38],\n",
      "        [1.0561e-38, 1.0286e-38, 1.0653e-38],\n",
      "        [1.0469e-38, 9.5510e-39, 9.9184e-39],\n",
      "        [9.0000e-39, 1.0561e-38, 1.0653e-38],\n",
      "        [4.1327e-39, 8.9082e-39, 9.8265e-39]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x=torch.empty(5,3)#5*3的未初始化的tensor\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0889, 0.5923, 0.2946],\n",
      "        [0.4116, 0.3431, 0.3291],\n",
      "        [0.2632, 0.8994, 0.8653],\n",
      "        [0.6747, 0.2467, 0.4824],\n",
      "        [0.8485, 0.5676, 0.8477]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.rand(5,3)#随机初始化的tensor\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(5,3,dtype=torch.long)#long型全0的tensor\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([5.5,3])#直接用数据创建\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[-5.1612e-01, -1.0816e-01,  1.4252e-03],\n",
      "        [-1.1978e-01,  1.1110e+00, -2.7205e-01],\n",
      "        [-4.6467e-02, -3.3716e-01, -1.6166e+00],\n",
      "        [-2.9310e-01,  1.2526e+00, -1.6284e+00],\n",
      "        [-6.2475e-01, -2.0883e+00,  5.1083e-01]])\n"
     ]
    }
   ],
   "source": [
    "x=x.new_ones(5,3,dtype=torch.float64) #使用已有的tensor来创建，默认重用一些属性\n",
    "print(x)\n",
    "\n",
    "x=torch.randn_like(x,dtype=torch.float)#制定新的数据类型\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.tensor的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2426,  0.5949,  0.0682],\n",
      "        [ 0.1195,  1.1386,  0.7269],\n",
      "        [ 0.1535, -0.1836, -0.6945],\n",
      "        [-0.0986,  2.2287, -1.4086],\n",
      "        [-0.1263, -1.7757,  0.6453]])\n"
     ]
    }
   ],
   "source": [
    "y=torch.rand(5,3)\n",
    "print(x+y)  #加法形式一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2426,  0.5949,  0.0682],\n",
      "        [ 0.1195,  1.1386,  0.7269],\n",
      "        [ 0.1535, -0.1836, -0.6945],\n",
      "        [-0.0986,  2.2287, -1.4086],\n",
      "        [-0.1263, -1.7757,  0.6453]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x,y))#:加法形式二"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2426,  0.5949,  0.0682],\n",
      "        [ 0.1195,  1.1386,  0.7269],\n",
      "        [ 0.1535, -0.1836, -0.6945],\n",
      "        [-0.0986,  2.2287, -1.4086],\n",
      "        [-0.1263, -1.7757,  0.6453]])\n"
     ]
    }
   ],
   "source": [
    "result=torch.empty(5,3)\n",
    "torch.add(x,y,out=result) #指定输出\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2426,  0.5949,  0.0682],\n",
      "        [ 0.1195,  1.1386,  0.7269],\n",
      "        [ 0.1535, -0.1836, -0.6945],\n",
      "        [-0.0986,  2.2287, -1.4086],\n",
      "        [-0.1263, -1.7757,  0.6453]])\n"
     ]
    }
   ],
   "source": [
    "# adds x to y\n",
    "y.add_(x) #加法形式三 inplace pytorch中inplace版本都有后缀_,如x.copy_(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 索引\n",
    "索引的结果与原数据共享内存，修改一个，另一个也会跟着修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4839, 1.8918, 2.0014])\n",
      "tensor([1.4839, 1.8918, 2.0014])\n"
     ]
    }
   ],
   "source": [
    "y=x[0,:]\n",
    "y += 1\n",
    "print(y)\n",
    "print(x[0,:]) #改变y,原有的x也会改变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 改变形状\n",
    "view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3]) torch.Size([15]) torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "y=x.view(15)\n",
    "z=x.view(-1,5) #-1所指的维度可以根据其他维度推算出来\n",
    "print(x.size(),y.size(),z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### view()返回的新Tensor与源Tensor虽然可能有不同的size，但是是共享data的，也即更改其中的一个，另外一个也会跟着改变。(顾名思义，view仅仅是改变了对这个张量的观察角度-视图，内部数据并未改变)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.4839,  2.8918,  3.0014],\n",
      "        [ 0.8802,  2.1110,  0.7280],\n",
      "        [ 0.9535,  0.6628, -0.6166],\n",
      "        [ 0.7069,  2.2526, -0.6284],\n",
      "        [ 0.3753, -1.0883,  1.5108]])\n",
      "tensor([ 2.4839,  2.8918,  3.0014,  0.8802,  2.1110,  0.7280,  0.9535,  0.6628,\n",
      "        -0.6166,  0.7069,  2.2526, -0.6284,  0.3753, -1.0883,  1.5108])\n"
     ]
    }
   ],
   "source": [
    "x += 1\n",
    "print(x)\n",
    "print(y)#y也会+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用clone()函数返回真正的副本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4839,  1.8918,  2.0014],\n",
      "        [-0.1198,  1.1110, -0.2720],\n",
      "        [-0.0465, -0.3372, -1.6166],\n",
      "        [-0.2931,  1.2526, -1.6284],\n",
      "        [-0.6247, -2.0883,  0.5108]])\n",
      "tensor([ 2.4839,  2.8918,  3.0014,  0.8802,  2.1110,  0.7280,  0.9535,  0.6628,\n",
      "        -0.6166,  0.7069,  2.2526, -0.6284,  0.3753, -1.0883,  1.5108])\n"
     ]
    }
   ],
   "source": [
    "x_cp=x.clone().view(15)\n",
    "x -= 1\n",
    "print(x)\n",
    "print(x_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用clone还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源Tensor。\n",
    "\n",
    "另外一个常用的常用的函数就是item(), 它可以将一个标量Tensor转换成一个Python number："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.2615])\n",
      "-1.2615078687667847\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性代数\n",
    "另外，PyTorch还支持一些线性函数，具体用法参考官方文档。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.广播机制\n",
    "之前内容是如何对两个形状相同的Tensor做按元素运算。当对两个形状不同的Tensor按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个Tensor形状相同后再按元素运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.arange(1,3).view(1,2)\n",
    "print(x)\n",
    "y=torch.arange(1,4).view(3,1)\n",
    "print(y)\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于x和y分别是1行2列和3行1列的矩阵，如果要计算x + y，那么x中第一行的2个元素被广播（复制）到了第二行和第三行，而y中第一列的3个元素被广播（复制）到了第二列。如此，就可以对2个3行2列的矩阵按元素相加。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.运算的内存开销\n",
    "索引操作是不会开辟新内存的，而像y = x + y这样的运算是会新开内存的，然后将y指向新内存。为了演示这一点，我们可以使用Python自带的id函数：如果两个实例的ID一致，那么它们所对应的内存地址相同；反之则不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([1,2])\n",
    "y=torch.tensor([3,4])\n",
    "id_before=id(y)\n",
    "y=y+x\n",
    "print(id(y)==id_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想指定结果到原来的y的内存，可以使用前面介绍的索引来进行替换操作。在下面的例子中，我们把x + y的结果通过[:]写进y对应的内存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([1,2])\n",
    "y=torch.tensor([3,4])\n",
    "id_before=id(y)\n",
    "y[:]=x+y\n",
    "print(id(y)==id_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以使用运算符全名函数中的out参数或者自加运算符+=(也即add_())达到上述效果，例如torch.add(x, y, out=y)和y += x(y.add_(x))。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([1,2])\n",
    "y=torch.tensor([3,4])\n",
    "id_before=id(y)\n",
    "torch.add(x,y,out=y) #y+=x, y.add_(x)\n",
    "print(id(y)==id_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Tensor和NumPy相互转换\n",
    "用numpy()和from_numpy()将Tensor和NumPy中的数组相互转换。但是需要注意的一点是： 这两个函数所产生的的Tensor和NumPy中的数组共享相同的内存（所以他们之间的转换很快），改变其中一个时另一个也会改变！！！\n",
    "\n",
    "还有一个常用的将NumPy中的array转换成Tensor的方法就是torch.tensor(), 需要注意的是，此方法总是会进行数据拷贝（就会消耗更多的时间和空间），所以返回的Tensor和原来的数据不再共享内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]\n",
      "tensor([3., 3., 3., 3., 3.]) [3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "a=torch.ones(5)\n",
    "b=a.numpy() #numpy()将tensor转为numpy数组\n",
    "print(a,b)\n",
    "a+=1 #共享内存\n",
    "print(a,b)\n",
    "b+=1\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.ones(5)\n",
    "b=torch.from_numpy(a)#from_numpy()将numpy数组转为tensor\n",
    "print(a,b)\n",
    "\n",
    "a+=1#共享内存\n",
    "print(a,b)\n",
    "b+=1\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 4. 4. 4. 4.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "c=torch.tensor(a)#直接使用torch.tensor()，会进行拷贝，a和c不再共享内存\n",
    "\n",
    "a += 1\n",
    "print(a,c) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Tensor on GPU\n",
    "用方法to()可以将Tensor在CPU和GPU（需要硬件支持）之间相互移动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.], device='cuda:0')\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x=torch.ones(5)\n",
    "# 以下代码只有在PyTorch GPU版本上才会执行\n",
    "if torch.cuda.is_available():\n",
    "    device=torch.device(\"cuda\") #GPU\n",
    "    y=torch.ones_like(x,device=device) #直接创建一个GPU上的tensor\n",
    "    x=x.to(device) #等价于 x.to(\"cuda\")\n",
    "    z=x+y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\",torch.double))#移动的同时也可以更改数据类型"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
